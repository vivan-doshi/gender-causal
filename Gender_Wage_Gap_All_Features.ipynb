{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02261099",
   "metadata": {},
   "source": [
    "# Gender Wage Gap Analysis: Comprehensive Feature Set\n",
    "\n",
    "This notebook performs a Double Machine Learning (DML) analysis of the gender wage gap using **all available variables** in the dataset. \n",
    "\n",
    "## Methodology\n",
    "1.  **Dynamic Data Cleaning**: Automatically remove low-quality columns (high missingness, zero variance).\n",
    "2.  **Redundancy Removal**: Identify and drop highly correlated features to prevent multicollinearity.\n",
    "3.  **Feature Engineering**: Create essential economic variables (Experience, Log Wage) while preserving the richness of the raw data.\n",
    "4.  **Double Machine Learning**: Estimate the causal effect of gender on wages controlling for the full, high-dimensional feature set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f024e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import doubleml as dml\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_excel('../data/raw/sample of sample ACS.xlsx')\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Data file not found. Please ensure 'sample of sample ACS.xlsx' is in '../data/raw/'.\")\n",
    "    # Create dummy data for demonstration if file is missing (so notebook can run)\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "    df = pd.DataFrame({\n",
    "        'INCWAGE': np.random.lognormal(10, 1, 1000),\n",
    "        'SEX': np.random.choice(['Male', 'Female'], 1000),\n",
    "        'AGE': np.random.randint(18, 65, 1000),\n",
    "        'EDUC': np.random.choice(['HS', 'College', 'Grad'], 1000),\n",
    "        'WKSWORK1': np.random.randint(40, 52, 1000),\n",
    "        'UHRSWORK': np.random.randint(30, 60, 1000),\n",
    "        'EMPSTAT': ['Employed'] * 1000\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28638dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Filtering\n",
    "# We only want employed individuals with positive wages\n",
    "print(\"Filtering for employed workers with positive wages...\")\n",
    "original_len = len(df)\n",
    "df = df[df['INCWAGE'] > 0]\n",
    "df = df[df['EMPSTAT'] == 'Employed']\n",
    "df = df[df['WKSWORK1'] > 0]\n",
    "df = df[df['UHRSWORK'] > 0]\n",
    "df = df[(df['AGE'] >= 18) & (df['AGE'] <= 65)]\n",
    "print(f\"Rows remaining: {len(df)} (Dropped {original_len - len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5fe9c",
   "metadata": {},
   "source": [
    "## Feature Engineering: Core Variables\n",
    "\n",
    "We create the fundamental variables required for the wage gap analysis:\n",
    "*   **`LOG_WAGE`**: The natural logarithm of annual wage income. This is standard in labor economics because wages are right-skewed and the log-linear model allows coefficients to be interpreted as percentage changes.\n",
    "*   **`FEMALE`**: A binary indicator (1 if Female, 0 if Male). This is our **treatment variable**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target and Treatment\n",
    "df['LOG_WAGE'] = np.log(df['INCWAGE'])\n",
    "df['FEMALE'] = (df['SEX'] == 'Female').astype(int)\n",
    "\n",
    "print(\"Created LOG_WAGE and FEMALE variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81854bcf",
   "metadata": {},
   "source": [
    "## Feature Engineering: Experience\n",
    "\n",
    "Since actual work experience is not in the dataset, we calculate **Potential Experience** using the Mincer equation approach:\n",
    "$$ \\text{Potential Experience} = \\text{Age} - \\text{Years of Education} - 6 $$\n",
    "\n",
    "We also include squared terms (`AGE_SQ`, `POTENTIAL_EXP_SQ`) to capture the non-linear relationship between age/experience and earnings (wages typically rise early in a career and flatten out).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Experience Variables\n",
    "df['AGE_SQ'] = df['AGE'] ** 2\n",
    "\n",
    "# Extract years of education (simplified mapping)\n",
    "# Assuming EDUCD or EDUC exists. If not, we skip.\n",
    "if 'EDUCD' in df.columns:\n",
    "    # Map EDUCD to approximate years (simplified)\n",
    "    df['EDUC_YEARS'] = pd.to_numeric(df['EDUCD'].astype(str).str[:1], errors='coerce').fillna(12)\n",
    "else:\n",
    "    df['EDUC_YEARS'] = 12 # Default\n",
    "\n",
    "df['POTENTIAL_EXP'] = df['AGE'] - df['EDUC_YEARS'] - 6\n",
    "df['POTENTIAL_EXP'] = df['POTENTIAL_EXP'].clip(lower=0) # No negative experience\n",
    "df['POTENTIAL_EXP_SQ'] = df['POTENTIAL_EXP'] ** 2\n",
    "\n",
    "print(\"Created Experience variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b3c70",
   "metadata": {},
   "source": [
    "## Dynamic Data Cleaning\n",
    "\n",
    "We now process **all other variables** in the dataset. To ensure the model is robust, we apply the following filters:\n",
    "\n",
    "1.  **High Missingness**: Drop columns with > 50% missing values.\n",
    "2.  **Zero Variance**: Drop columns that have only 1 unique value (they provide no information).\n",
    "3.  **Object Conversion**: Convert all categorical (object) columns to numeric using Label Encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop High Missingness\n",
    "missing_threshold = 0.5\n",
    "missing_ratio = df.isnull().mean()\n",
    "drop_missing = missing_ratio[missing_ratio > missing_threshold].index.tolist()\n",
    "df = df.drop(columns=drop_missing)\n",
    "print(f\"Dropped columns with > {missing_threshold*100}% missing values: {drop_missing}\")\n",
    "\n",
    "# 2. Drop Zero Variance (Single Value)\n",
    "nunique = df.nunique()\n",
    "drop_single = nunique[nunique <= 1].index.tolist()\n",
    "df = df.drop(columns=drop_single)\n",
    "print(f\"Dropped {len(drop_single)} columns with only 1 unique value.\")\n",
    "\n",
    "# 3. Handle Remaining Missing Values\n",
    "# Fill numeric with 0, object with 'Unknown'\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "obj_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "df[obj_cols] = df[obj_cols].fillna('Unknown')\n",
    "\n",
    "# 4. Label Encode Categorical Variables\n",
    "le = LabelEncoder()\n",
    "for col in obj_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "print(f\"Processed {len(obj_cols)} categorical columns.\")\n",
    "print(f\"Final Data Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af0fed",
   "metadata": {},
   "source": [
    "## Redundancy Removal\n",
    "\n",
    "We calculate the correlation matrix for all features. If two features have a correlation greater than **0.95**, they are effectively redundant. We drop one of them to reduce dimensionality and improve model stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8595645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Correlation Matrix\n",
    "# (Using a subset or efficient method if data is huge, but here we do full)\n",
    "print(\"Calculating correlations...\")\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation > 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Keep essential variables even if correlated\n",
    "essentials = ['LOG_WAGE', 'FEMALE', 'AGE', 'AGE_SQ', 'POTENTIAL_EXP', 'POTENTIAL_EXP_SQ']\n",
    "to_drop = [col for col in to_drop if col not in essentials]\n",
    "\n",
    "df = df.drop(columns=to_drop)\n",
    "print(f\"Dropped {len(to_drop)} highly correlated features (>0.95).\")\n",
    "print(f\"Remaining columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e30121",
   "metadata": {},
   "source": [
    "## Variable Explanation\n",
    "\n",
    "The following table lists the variables used in the analysis after cleaning and redundancy removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9abfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Explanation\n",
    "feature_info = []\n",
    "for col in df.columns:\n",
    "    if col not in ['LOG_WAGE', 'FEMALE', 'INCWAGE', 'SEX']:\n",
    "        dtype = str(df[col].dtype)\n",
    "        feature_info.append({'Feature': col, 'Type': dtype})\n",
    "\n",
    "feature_df = pd.DataFrame(feature_info)\n",
    "from IPython.display import display, Markdown\n",
    "print(f\"Total Features Used: {len(feature_df)}\")\n",
    "display(feature_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18426c5",
   "metadata": {},
   "source": [
    "## Double Machine Learning Analysis\n",
    "\n",
    "We now run the DoubleML Partial Linear Regression (`DoubleMLPLR`) model.\n",
    "\n",
    "*   **Outcome ($Y$)**: `LOG_WAGE`\n",
    "*   **Treatment ($D$)**: `FEMALE`\n",
    "*   **Controls ($X$)**: All other remaining variables.\n",
    "\n",
    "We use **LightGBM** as the machine learning learner for both the nuisance functions (predicting wage and predicting gender). LightGBM is chosen for its speed and ability to handle large, high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features\n",
    "y_col = 'LOG_WAGE'\n",
    "d_col = 'FEMALE'\n",
    "x_cols = [c for c in df.columns if c not in [y_col, d_col, 'INCWAGE', 'SEX']] # Exclude raw target/treatment\n",
    "\n",
    "print(f\"Using {len(x_cols)} control variables.\")\n",
    "\n",
    "# Initialize DoubleML Data\n",
    "dml_data = dml.DoubleMLData(df,\n",
    "                           y_col=y_col,\n",
    "                           d_cols=d_col,\n",
    "                           x_cols=x_cols)\n",
    "\n",
    "# Initialize Learners\n",
    "# We use LightGBM for speed and performance\n",
    "ml_l = LGBMRegressor(n_estimators=100, learning_rate=0.1, n_jobs=-1, verbose=-1)\n",
    "ml_m = LGBMClassifier(n_estimators=100, learning_rate=0.1, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Initialize DoubleML Model\n",
    "np.random.seed(42)\n",
    "dml_plr = dml.DoubleMLPLR(dml_data,\n",
    "                          ml_l=ml_l,\n",
    "                          ml_m=ml_m,\n",
    "                          n_folds=3)\n",
    "\n",
    "# Fit Model\n",
    "print(\"Fitting DoubleML model (this may take a moment)...\")\n",
    "dml_plr.fit()\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DOUBLE ML RESULTS (ALL FEATURES)\")\n",
    "print(\"=\"*50)\n",
    "print(dml_plr.summary)\n",
    "\n",
    "gap_pct = (np.exp(dml_plr.coef[0]) - 1) * 100\n",
    "print(f\"\\nEstimated Gender Wage Gap: {gap_pct:.2f}%\")\n",
    "print(f\"Women earn {100 + gap_pct:.1f} cents for every dollar men earn (adjusted for all features).\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
